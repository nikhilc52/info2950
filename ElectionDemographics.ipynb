{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 2950 Final Project - Phase II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Questions:\n",
    "\n",
    "Questions: Can we predict the probability that a voting-eligible person living in the United States will vote based on the year and their state’s voter turnout and population density, literacy rate, age, race, and income? Are there any noticeable patterns or correlations between each of these variables (Ex. does a higher literacy rate increase the likelihood that an person votes)? How do these voting trends and correlations change over time (Ex. as literacy rates increases over time, do voting trends follow)? \n",
    "\n",
    "The main differentiating feature of our project is that we will be conducting this analysis on demographics and voting patterns by each state and each election year, instead of nationally.\n",
    "\n",
    "In this assignment, we will train a multivariable regression model to determine if we can reliably predict how likely a voter is to vote in their general (mid-term) election (so every two years) given the stated factors above. We set out to analyze how socioeconomic factors as well as the changing political climate over the year has affected Americans’ willingness to vote. To narrow the focus of our project, we're only looking at data from the 2000 election to the 2022 election (every two years), which allows us to connect voting data to other datasets that would likely be more available for modern years (it's difficult to find each state's literacy rate for every year dating back to 1950, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Descriptions:\n",
    "\n",
    "The data we use comes from a number of different sources. Each of these dataframes is then combined (using the appropriate merge commands) into a single dataframe, which is then easier to conduct analysis on (and visualize). This merged data frame will have the following columns: TODO\n",
    "\n",
    "Here are short descriptions of the data sources:\n",
    " - Voter Turnout Data: https://www.electproject.org/election-data/voter-turnout-data\n",
    "    - This website was created by Michael P. McDonald, a professor of political science at the University of Florida. It is just a host for the data displayed on https://election.lab.ufl.edu/voter-turnout/ (so a reputable source). To get the data in the form we wanted, we downloaded 10 CSVs from this site (one for each election) and merged them vertically. The data is usable under the Creative Commons Attribution 3.0 Unported License (with proper citation).\n",
    "\n",
    " - Bachelor's Degree or Higher by State: https://fred.stlouisfed.org/release/tables?rid=330&eid=391444&od=2009-01-01# \n",
    " - Real Median Household Income by State: https://fred.stlouisfed.org/release/tables?rid=249&eid=259515&od=# \n",
    " - Resident Population by State: https://fred.stlouisfed.org/release/tables?rid=118&eid=259194\n",
    "   - This website is FRED, which is one of the most reputable sources for economic data. To get the data in the form we wanted (merged), we used an importHTML formula in Google Sheets and downloaded that data as a CSV. The data is usable with proper citation, according to the legal notice on the site.\n",
    "\n",
    " - Race Makeup by State: https://www.kff.org/other/state-indicator/distribution-by-raceethnicity/?currentTimeframe=0&sortModel=%7B%22colId%22:%22Location%22,%22sort%22:%22asc%22%7D\n",
    " - Age by State: https://www.kff.org/other/state-indicator/distribution-by-age/?dataView=1&activeTab=graph&currentTimeframe=0&startTimeframe=13&sortModel=%7B%22colId%22:%22Location%22,%22sort%22:%22asc%22%7D\n",
    "    - This website is KFF, which hosts the \"most up-to-date and accurate information on health policy\". We downloaded the data as a single CSV from this site, and imported in directly into our project. The data is usable with proper citation, according to the citation page on the site.\n",
    "\n",
    " - Area by State: https://www.census.gov/geographies/reference-files/2010/geo/state-area.html\n",
    "   - This website is the US Census Bureau, which is as formal as it gets for data about US demographics. The data is usable with proper citation, according to the citation page on the site.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning:\n",
    "\n",
    "To work towards our goal of a single merged data frame, we need to load in each CSV and clean it before merging everything together using SQL.\n",
    "\n",
    "We started by downloading the Voter Turnout Data from [electproject.org ](https://www.electproject.org/election-data/voter-turnout-data) and using Excel to quickly fix errors in the data. \n",
    "\n",
    "These errors included certain CSVs (2016, 2018, 2020, and 2022) containing different names for the same column values, which would cause errors when trying to vertically join the data. There were also a few states that had '*' at the end of their names, which corresponded to certain (irrelevant for our use) notes that were held at the bottom of the CSV. We removed those in Excel, and deleted the rows containing the notes as well, since pandas would read those in as additional rows in our dataframe.\n",
    "\n",
    "When reading in the data from CSV form, we found it easy to skip rows at index 0 and 2, corresponding to unneeded headers and the State 'United States' in the data. Note that if we need data for the whole US later, we can make it ourselves using simple grouping and addition. We also parsed in commas to read the data.\n",
    "\n",
    "Since a number of columns were not needed, we used an SQL statement to rename columns to shorter, more descriptive names and added two columns for percentages as floats. There are some NaNs in the data, but since this is going to be the left most data set in our merge calls, we want there to be a value for each state in each year, so we'll leave them as is for now.\n",
    "\n",
    "As a sanity check, since D.C. is included as a state, we should have 51 states * 11 elections = 561 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>TotalBallots</th>\n",
       "      <th>VotingEligiblePopulation</th>\n",
       "      <th>VotingAgePopulation</th>\n",
       "      <th>PercentVotingEligibleVotes</th>\n",
       "      <th>PercentVotingAgeVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3241682</td>\n",
       "      <td>3334576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2000</td>\n",
       "      <td>287825.0</td>\n",
       "      <td>419111</td>\n",
       "      <td>440296</td>\n",
       "      <td>0.686751</td>\n",
       "      <td>0.653708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>2000</td>\n",
       "      <td>1559520.0</td>\n",
       "      <td>3357701</td>\n",
       "      <td>3816498</td>\n",
       "      <td>0.464461</td>\n",
       "      <td>0.408626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1925961</td>\n",
       "      <td>2001774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>2000</td>\n",
       "      <td>11142843.0</td>\n",
       "      <td>19685258</td>\n",
       "      <td>24867252</td>\n",
       "      <td>0.566050</td>\n",
       "      <td>0.448093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>2022</td>\n",
       "      <td>3021956.0</td>\n",
       "      <td>6348891</td>\n",
       "      <td>6836463</td>\n",
       "      <td>0.475982</td>\n",
       "      <td>0.442035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2022</td>\n",
       "      <td>3067686.0</td>\n",
       "      <td>5543001</td>\n",
       "      <td>6164761</td>\n",
       "      <td>0.553434</td>\n",
       "      <td>0.497616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>2022</td>\n",
       "      <td>494753.0</td>\n",
       "      <td>1396169</td>\n",
       "      <td>1423003</td>\n",
       "      <td>0.354365</td>\n",
       "      <td>0.347682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2022</td>\n",
       "      <td>2673154.0</td>\n",
       "      <td>4467396</td>\n",
       "      <td>4655496</td>\n",
       "      <td>0.598370</td>\n",
       "      <td>0.574193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2022</td>\n",
       "      <td>198198.0</td>\n",
       "      <td>434797</td>\n",
       "      <td>452896</td>\n",
       "      <td>0.455840</td>\n",
       "      <td>0.437624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             State  Year  TotalBallots  VotingEligiblePopulation  \\\n",
       "0          Alabama  2000           NaN                   3241682   \n",
       "1           Alaska  2000      287825.0                    419111   \n",
       "2          Arizona  2000     1559520.0                   3357701   \n",
       "3         Arkansas  2000           NaN                   1925961   \n",
       "4       California  2000    11142843.0                  19685258   \n",
       "..             ...   ...           ...                       ...   \n",
       "607       Virginia  2022     3021956.0                   6348891   \n",
       "608     Washington  2022     3067686.0                   5543001   \n",
       "609  West Virginia  2022      494753.0                   1396169   \n",
       "610      Wisconsin  2022     2673154.0                   4467396   \n",
       "611        Wyoming  2022      198198.0                    434797   \n",
       "\n",
       "     VotingAgePopulation  PercentVotingEligibleVotes  PercentVotingAgeVotes  \n",
       "0                3334576                         NaN                    NaN  \n",
       "1                 440296                    0.686751               0.653708  \n",
       "2                3816498                    0.464461               0.408626  \n",
       "3                2001774                         NaN                    NaN  \n",
       "4               24867252                    0.566050               0.448093  \n",
       "..                   ...                         ...                    ...  \n",
       "607              6836463                    0.475982               0.442035  \n",
       "608              6164761                    0.553434               0.497616  \n",
       "609              1423003                    0.354365               0.347682  \n",
       "610              4655496                    0.598370               0.574193  \n",
       "611               452896                    0.455840               0.437624  \n",
       "\n",
       "[612 rows x 7 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creates a years list of all the election years we have data for\n",
    "years = np.linspace(2000, 2022, 12)\n",
    "#initialize a dictionary\n",
    "d = {}\n",
    "#cycle through each of the years, adding the year as the key mapped to the dataframe for each year\n",
    "for year in years:\n",
    "    #skip rows for headers/US, parse thousands\n",
    "    d[int(year)] = pd.read_csv('data/election_'+str(int(year))+'.csv', skiprows=[0,2] , thousands=\",\")\n",
    "    #make a column for the year\n",
    "    d[int(year)]['Year'] = int(year)\n",
    "\n",
    "#make a list to store the dataframes\n",
    "df_list = []\n",
    "#for each year in the dictionary (for each year)\n",
    "for year in d.items():\n",
    "    #append the dataframe to the list\n",
    "    df_list.append(year[1])\n",
    "\n",
    "#concat all the dataframes together, ignoring index\n",
    "election_year_data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "#print the shape as a sanity check, should number of rows should be 612 = 12*51\n",
    "print(election_year_data.shape)\n",
    "\n",
    "#use SQL to change certain column names and create 2 new columns for percentages\n",
    "election_year_data = duckdb.sql(\"\"\"SELECT \"Unnamed: 0\" AS State, Year, \"Total Ballots Counted\" AS TotalBallots,\n",
    "                                \"Voting-Eligible Population (VEP)\" AS VotingEligiblePopulation, \n",
    "                                \"Voting-Age Population (VAP)\" AS VotingAgePopulation, \n",
    "                                TotalBallots/VotingEligiblePopulation AS PercentVotingEligibleVotes,\n",
    "                                TotalBallots/VotingAgePopulation AS PercentVotingAgeVotes\n",
    "                                FROM election_year_data\"\"\").df()\n",
    "\n",
    "# #print the dataset to verify completion and accuracy\n",
    "election_year_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now loading in data about the education level (specifically, the percent of the population that has a Bachelor's degree or higher) of each state in each election year. This was scraped using importHTML in Google Sheets from [FRED](https://fred.stlouisfed.org/release/tables?rid=330&eid=391444&od=2008-01-01#).\n",
    "\n",
    "As a general overview of how web scraping in Google Sheets works, we supply a link to the importHTML formula, specify what type of data type we want to parse ('table' in this case), and the number (what table number to scrape) as the final parameter. We can then copy the column we need (% bachelor's) into a new sheet, aligning it with the state the data corresponds to. Note that the link to the FRED website uses a date as a parameter, meaning we can treat our importHTML function as a kind of API. Essentially, we can edit the date to the specific year we want, and the data will automatically be updated in Google Sheets, which allows us to do a bunch of copy-pasting of data, without ever having to download and merge CSVs until we have our final sheet. This is a kind of web scraping that is quasi-automatic and similar to an API call, and significantly easier than downloading and merging 10 CSVs in Python. Note that this is only possible on FRED sites, since they display the data in tables on the web, rather than the other sites which don't do the same (or do it in a way we can't web scrape).\n",
    "\n",
    "Unfortunately, we don't have data pre-2006 for each state. When we merge everything together, we'll use a left join and deal with the NaNs later.\n",
    "\n",
    "The data from FRED comes in wide form, but we want to get it into long so we can merge everything together. So, after we read in the CSV, we'll use pd.melt to move the data into long form. Since we started with 51 rows (D.C. is a state) and have 8 columns/years we want to melt, we should end up with 8 elections * 51 states (D.C.) = 408 rows.\n",
    "\n",
    "Most of the code parsing data from FRED will follow this form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>PercentBachelors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2006</td>\n",
       "      <td>21.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2006</td>\n",
       "      <td>26.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>2006</td>\n",
       "      <td>25.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2006</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>2006</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>2022</td>\n",
       "      <td>42.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2022</td>\n",
       "      <td>39.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>2022</td>\n",
       "      <td>24.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2022</td>\n",
       "      <td>33.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2022</td>\n",
       "      <td>29.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  Year  PercentBachelors\n",
       "0          Alabama  2006              21.1\n",
       "1           Alaska  2006              26.9\n",
       "2          Arizona  2006              25.5\n",
       "3         Arkansas  2006              18.2\n",
       "4       California  2006              29.0\n",
       "..             ...   ...               ...\n",
       "403       Virginia  2022              42.2\n",
       "404     Washington  2022              39.5\n",
       "405  West Virginia  2022              24.8\n",
       "406      Wisconsin  2022              33.2\n",
       "407        Wyoming  2022              29.6\n",
       "\n",
       "[408 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#read in the data\n",
    "education = pd.read_csv('data/education.csv', thousands=\",\")\n",
    "#melt the data into long form\n",
    "education = education.melt(id_vars=['Name'],value_name=\"PercentBachelors\",var_name=\"Year\")\n",
    "#printing the shape should yield 408 rows\n",
    "print(education.shape)\n",
    "#print education to verify\n",
    "education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now loading in data about the income level (specifically, the Real Median Household Income by State) of each state in each election year. This was scraped using importHTML in Google Sheets from [FRED](https://fred.stlouisfed.org/release/tables?rid=249&eid=259515&od=#).\n",
    "\n",
    "We followed the same steps as the above (education) to parse the data before reading it into Python.\n",
    "\n",
    "The data from FRED comes in wide form, but we want to get it into long so we can merge everything together. So, after we read in the CSV, we'll use pd.melt to move the data into long form. Since we started with 51 rows (D.C. is a state) and have 12 columns/years we want to melt, we should end up with 12 elections * 51 states (D.C.) = 612 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2000</td>\n",
       "      <td>4452.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2000</td>\n",
       "      <td>627.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>2000</td>\n",
       "      <td>5160.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2000</td>\n",
       "      <td>2678.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>2000</td>\n",
       "      <td>33987.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>2022</td>\n",
       "      <td>8679.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2022</td>\n",
       "      <td>7784.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>2022</td>\n",
       "      <td>1774.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2022</td>\n",
       "      <td>5890.540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2022</td>\n",
       "      <td>581.629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  Year     Income\n",
       "0          Alabama  2000   4452.170\n",
       "1           Alaska  2000    627.963\n",
       "2          Arizona  2000   5160.590\n",
       "3         Arkansas  2000   2678.590\n",
       "4       California  2000  33987.980\n",
       "..             ...   ...        ...\n",
       "607       Virginia  2022   8679.100\n",
       "608     Washington  2022   7784.480\n",
       "609  West Virginia  2022   1774.040\n",
       "610      Wisconsin  2022   5890.540\n",
       "611        Wyoming  2022    581.629\n",
       "\n",
       "[612 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the csv\n",
    "income = pd.read_csv('data/income.csv', thousands=\",\")\n",
    "#melt the data into long form\n",
    "income = income.melt(id_vars=['Name'],value_name=\"Income\",var_name=\"Year\")\n",
    "#print the shape of the dataframe as a sanity check, should be 612 rows\n",
    "print(income.shape)\n",
    "#print the data frame to make sure everything looks right\n",
    "income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now loading in data about the racial demographics from [KFF.org](https://www.kff.org/other/state-indicator/distribution-by-raceethnicity/?currentTimeframe=0&sortModel=%7B%22colId%22:%22Location%22,%22sort%22:%22asc%22%7D). To obtain a single numeric value for something as abstract as \"diversity\", we simply found the percent of the population that is white, which we can then use to obtain the percent of population that is a minority racial group, if needed. This is a big abstraction to make, but needed to obtain quantitative data relatively simply.\n",
    "\n",
    "The initial CSV has the U.S. as a row, which we're excluding (unfortunately, since the values here are percentages, we can't re-derive this, but we can easily come back to this point to include the US if needed). Also unfortunate is the fact that 2020 is not included in this data.\n",
    "\n",
    "The data from KFF comes in wide form, but we want to get it into long so we can merge everything together. So, after we read in the CSV, we'll use pd.melt to move the data into long form. Since we started with 51 rows (D.C. is a state) and have 14 columns/years we want to melt, we should end up with 14 elections * 51 states (D.C.) = 714 rows.\n",
    "\n",
    "However, the since the original column names had _ _ White appended to the year (i.e. 2008__White), this values bleed into the column values for the 'Year' column we just created. We can't have this, though, since we need to merge this data back based on Location and Year. So, we'll use a simple REGEX formula to replace everything before the \"_ _\" with nothing. This way, the Year column just contains a single number. Note that we're doing this after we melt, since it's easier to rename all the values in a single column then rename 14 different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Year</th>\n",
       "      <th>PercentWhite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.68670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.66160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.58220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.75710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.42030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.58904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.63597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.90436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.79052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.81906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Location  Year  PercentWhite\n",
       "0          Alabama  2008       0.68670\n",
       "1           Alaska  2008       0.66160\n",
       "2          Arizona  2008       0.58220\n",
       "3         Arkansas  2008       0.75710\n",
       "4       California  2008       0.42030\n",
       "..             ...   ...           ...\n",
       "709       Virginia  2022       0.58904\n",
       "710     Washington  2022       0.63597\n",
       "711  West Virginia  2022       0.90436\n",
       "712      Wisconsin  2022       0.79052\n",
       "713        Wyoming  2022       0.81906\n",
       "\n",
       "[714 rows x 3 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the CSV, skipping the row for the US as a whole\n",
    "diversity = pd.read_csv('data/diversity.csv', thousands=\",\", skiprows=[1])\n",
    "#melt the data into long form\n",
    "diversity = diversity.melt(id_vars=['Location'],value_name=\"PercentWhite\",var_name=\"Year\")\n",
    "#print the shape as a sanity check, should have 714 rows\n",
    "print(diversity.shape)\n",
    "#fix the year column by only looking at the stuff before __\n",
    "diversity['Year'] = diversity['Year'].replace(r'(.*)__.*', r'\\1', regex=True)\n",
    "#print out the data frame to ensure correctness\n",
    "diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load in data about the mean age for each state. This data is also from [KFF.org](https://www.kff.org/other/state-indicator/distribution-by-age/?dataView=1&activeTab=graph&currentTimeframe=0&startTimeframe=13&sortModel=%7B%22colId%22:%22Location%22,%22sort%22:%22asc%22%7D). \n",
    "\n",
    "As we read in the CSV, we need to get rid of the rows at index 0, 1, and 3, since they contain unneeded headers and US.\n",
    "\n",
    "Unfortunately, the data doesn't contain a single number for median or mean age, but includes the number of people in a number of age ranges, as well as the total number of people in the state (for each year). So, we'll need to manually calculate (estimate, really) this value for each state in each year. It wouldn't be efficient to copy and paste creating a new column 14 times (making a new column for each year and there are 14 years), so we can speed up the process using a for loop.\n",
    "\n",
    "The loop cycles through each of the years present in the data (unfortunately, no 2020). For each year, we make a new column in the data frame with the year as its title (along with a description). Then, we assign the sum of all the ages to the column. Since we're given the number of people in each age range, we simply multiply that value by the median value of the age range to create an estimate of the sum of ages. Next, we divide the column by the total number of people in the state to find the average age for each state. Since this is in a loop, we do this for all the years, too.\n",
    "\n",
    "We're also getting rid of the age range columns, since we don't care about them now that we've found an average age.\n",
    "\n",
    "The data from KFF comes in wide form, but we want to get it into long so we can merge everything together. So, after we read in the CSV, we'll use pd.melt to move the data into long form. \n",
    "\n",
    "However, the since the original column names had _ _ White appended to the year (i.e. 2008__White), this values bleed into the column values for the 'Year' column we just created. We can't have this, though, since we need to merge this data back based on Location and Year. So, we'll use a simple REGEX formula to replace everything before the \"_ _\" with nothing. This way, the Year column just contains a single number. Note that we're doing this after we melt, since it's easier to rename all the values in a single column then rename 14 different columns.\n",
    "\n",
    "Before we finish with this dataframe, we need to get rid of the rows corresponding to Puerto Rico, since that's not included in our other datasets. Really, we don't *need* to do this step (since this we won't be left joining on this data), but it's useful to tidy up our data. After we filter out those values, we reset the index and verify that since we started with 51 rows (D.C. is a state) and have 14 columns/years we want to melt, we should end up with 7 elections * 51 states (D.C.) = 357 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Year</th>\n",
       "      <th>AverageAge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2008</td>\n",
       "      <td>37.655371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2008</td>\n",
       "      <td>34.414999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>2008</td>\n",
       "      <td>36.578626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2008</td>\n",
       "      <td>37.745749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>2008</td>\n",
       "      <td>35.884491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>2022</td>\n",
       "      <td>40.044919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2022</td>\n",
       "      <td>39.907804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>2022</td>\n",
       "      <td>42.397487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2022</td>\n",
       "      <td>40.930317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2022</td>\n",
       "      <td>40.389528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Location  Year  AverageAge\n",
       "0          Alabama  2008   37.655371\n",
       "1           Alaska  2008   34.414999\n",
       "2          Arizona  2008   36.578626\n",
       "3         Arkansas  2008   37.745749\n",
       "4       California  2008   35.884491\n",
       "..             ...   ...         ...\n",
       "352       Virginia  2022   40.044919\n",
       "353     Washington  2022   39.907804\n",
       "354  West Virginia  2022   42.397487\n",
       "355      Wisconsin  2022   40.930317\n",
       "356        Wyoming  2022   40.389528\n",
       "\n",
       "[357 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the csv, ignoring rows for headers and the US\n",
    "age = pd.read_csv('data/age.csv', thousands=\",\", skiprows=[0,1,3])\n",
    "#make a list of years we need to find averages for, excluding 2020\n",
    "list_years = np.arange(2008,2020)\n",
    "list_years = np.append(list_years, [2021,2022])\n",
    "#for each value in the list\n",
    "for i in list_years:\n",
    "    #make a new column based on the year and populate it with an estimate of the sums of ages\n",
    "    age[str(i)+'__EstMeanAge'] = ((age[str(i)+'__Children 0-18']*9) + (age[str(i)+'__Adults 19-25']*22) + (age[str(i)+'__Adults 26-34']*30)\n",
    "                                  + (age[str(i)+'__Adults 35-54']*45) + (age[str(i)+'__Adults 55-64']*60) + (age[str(i)+'__65+']*75))\n",
    "    #divide the sum of the ages by the total number of people to find an average\n",
    "    age[str(i)+'__EstMeanAge'] = (age[str(i)+'__EstMeanAge']) / (age[str(i)+'__Total'])\n",
    "    \n",
    "#only select certain columns with the averages\n",
    "age = age.loc[:,['Location','2008__EstMeanAge','2010__EstMeanAge','2012__EstMeanAge','2014__EstMeanAge',\n",
    "                  '2016__EstMeanAge','2018__EstMeanAge','2022__EstMeanAge']]\n",
    "#melt the dataframe into long format\n",
    "age = age.melt(id_vars=['Location'],value_name=\"AverageAge\",var_name=\"Year\")\n",
    "#replace the values in the 'Year' column with just numbers\n",
    "age['Year'] = age['Year'].replace(r'(.*)__.*', r'\\1', regex=True)\n",
    "#remove puerto rico from the dataset, since we don't have voting data from there, so we only need 51 states (D.C.)\n",
    "age = age[age['Location'] != 'Puerto Rico']\n",
    "#reset the index\n",
    "age = age.reset_index(drop=True)\n",
    "#print the shape of the dataframe, should have 357 rows\n",
    "print(age.shape)\n",
    "#print the dataframe to verify accuracy\n",
    "age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataframe that we need to load in is for the population and later the area, in order to find population density. This data comes from [FRED](https://fred.stlouisfed.org/release/tables?rid=118&eid=259194), so we performed the same Google Sheets web scraping to obtain the data. \n",
    "\n",
    "The data from FRED comes in wide form, but we want to get it into long so we can merge everything together. So, after we read in the CSV, we'll use pd.melt to move the data into long form. Since we started with 51 rows (D.C. is a state) and have 12 columns/years we want to melt, we should end up with 12 elections * 51 states (D.C.) = 612 rows.\n",
    "\n",
    "Next, we need to load in he area of each state. This data was Google-Sheets-Web-Scraped from [census.gov](https://www.census.gov/geographies/reference-files/2010/geo/state-area.html), and loaded directly into Python.\n",
    "\n",
    "Now, we'll join the two datasets together to find the population density, in terms of people per square mile. We left join on the population dataframe, so that we have a land value for each year and put this merged dataframe into a new variable. Then, we can create a new column with the population density per square mile. Since the population data was measured in terms of 1k people, we need to multiple this by 1000 to get people/sq mi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Population</th>\n",
       "      <th>LandAreaSQM</th>\n",
       "      <th>PopulationDensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2000</td>\n",
       "      <td>627.963</td>\n",
       "      <td>570641</td>\n",
       "      <td>1.100452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>2000</td>\n",
       "      <td>5160.590</td>\n",
       "      <td>113594</td>\n",
       "      <td>45.430128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2000</td>\n",
       "      <td>2678.590</td>\n",
       "      <td>52035</td>\n",
       "      <td>51.476698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California</td>\n",
       "      <td>2000</td>\n",
       "      <td>33987.980</td>\n",
       "      <td>155779</td>\n",
       "      <td>218.180756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>2000</td>\n",
       "      <td>4326.920</td>\n",
       "      <td>103642</td>\n",
       "      <td>41.748712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>2018</td>\n",
       "      <td>6042.150</td>\n",
       "      <td>9707</td>\n",
       "      <td>622.452869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020</td>\n",
       "      <td>5031.860</td>\n",
       "      <td>50645</td>\n",
       "      <td>99.355514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>2020</td>\n",
       "      <td>6173.690</td>\n",
       "      <td>9707</td>\n",
       "      <td>636.003915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2022</td>\n",
       "      <td>5073.900</td>\n",
       "      <td>50645</td>\n",
       "      <td>100.185606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>2022</td>\n",
       "      <td>6163.980</td>\n",
       "      <td>9707</td>\n",
       "      <td>635.003606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name  Year  Population  LandAreaSQM  PopulationDensity\n",
       "0        Alaska  2000     627.963       570641           1.100452\n",
       "1       Arizona  2000    5160.590       113594          45.430128\n",
       "2      Arkansas  2000    2678.590        52035          51.476698\n",
       "3    California  2000   33987.980       155779         218.180756\n",
       "4      Colorado  2000    4326.920       103642          41.748712\n",
       "..          ...   ...         ...          ...                ...\n",
       "607    Maryland  2018    6042.150         9707         622.452869\n",
       "608     Alabama  2020    5031.860        50645          99.355514\n",
       "609    Maryland  2020    6173.690         9707         636.003915\n",
       "610     Alabama  2022    5073.900        50645         100.185606\n",
       "611    Maryland  2022    6163.980         9707         635.003606\n",
       "\n",
       "[612 rows x 5 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the population data\n",
    "population = pd.read_csv('data/population.csv', thousands=\",\")\n",
    "#melt the data into long form\n",
    "population = population.melt(id_vars=['Name'],value_name=\"Population\",var_name=\"Year\")\n",
    "#print the shape for sanity, should have 612 rows\n",
    "print(population.shape)\n",
    "\n",
    "#read in the area data\n",
    "area = pd.read_csv('data/area.csv', thousands=\",\")\n",
    "\n",
    "#make a new dataframe with the population and area for each state in each year\n",
    "population_density = duckdb.sql(\"\"\"SELECT Name,Year,Population,LandAreaSQM \n",
    "                                FROM population LEFT JOIN area ON population.Name = area.State\"\"\").df()\n",
    "\n",
    "#make a new column based on the population density calculation (people/sqmi)\n",
    "population_density['PopulationDensity'] = 1000*population_density['Population']/population_density['LandAreaSQM']\n",
    "#print the dataframe to make sure its accurate\n",
    "population_density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to merge each of these data frames together. For completeness, we have 6 total dataframes (election_year_data, education, income, diversity, age, population_density). We're going to be doing 5 left joins onto election_year_data, so that for every state and every year, we have the corresponding data (or NaNs).\n",
    "\n",
    "Note that we're making sure to not include duplicate values, since we want our data to be as compact as possible. Also, duplicate columns would multiply as we keep adding more left joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>TotalBallots</th>\n",
       "      <th>VotingEligiblePopulation</th>\n",
       "      <th>VotingAgePopulation</th>\n",
       "      <th>PercentVotingEligibleVotes</th>\n",
       "      <th>PercentVotingAgeVotes</th>\n",
       "      <th>PercentBachelors</th>\n",
       "      <th>Income</th>\n",
       "      <th>PercentWhite</th>\n",
       "      <th>AverageAge</th>\n",
       "      <th>Population</th>\n",
       "      <th>LandAreaSQM</th>\n",
       "      <th>PopulationDensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2008</td>\n",
       "      <td>2105622.0</td>\n",
       "      <td>3454510</td>\n",
       "      <td>3595708</td>\n",
       "      <td>0.609528</td>\n",
       "      <td>0.585593</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4718.210</td>\n",
       "      <td>0.68670</td>\n",
       "      <td>37.655371</td>\n",
       "      <td>4718.210</td>\n",
       "      <td>50645</td>\n",
       "      <td>93.162405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2008</td>\n",
       "      <td>327341.0</td>\n",
       "      <td>479429</td>\n",
       "      <td>507645</td>\n",
       "      <td>0.682773</td>\n",
       "      <td>0.644823</td>\n",
       "      <td>27.3</td>\n",
       "      <td>687.455</td>\n",
       "      <td>0.66160</td>\n",
       "      <td>34.414999</td>\n",
       "      <td>687.455</td>\n",
       "      <td>570641</td>\n",
       "      <td>1.204707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2008</td>\n",
       "      <td>1095958.0</td>\n",
       "      <td>2071563</td>\n",
       "      <td>2174846</td>\n",
       "      <td>0.529049</td>\n",
       "      <td>0.503924</td>\n",
       "      <td>18.8</td>\n",
       "      <td>2874.550</td>\n",
       "      <td>0.75710</td>\n",
       "      <td>37.745749</td>\n",
       "      <td>2874.550</td>\n",
       "      <td>52035</td>\n",
       "      <td>55.242625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>2008</td>\n",
       "      <td>2422236.0</td>\n",
       "      <td>3382959</td>\n",
       "      <td>3708955</td>\n",
       "      <td>0.716011</td>\n",
       "      <td>0.653078</td>\n",
       "      <td>35.6</td>\n",
       "      <td>4889.730</td>\n",
       "      <td>0.71060</td>\n",
       "      <td>36.314987</td>\n",
       "      <td>4889.730</td>\n",
       "      <td>103642</td>\n",
       "      <td>47.179039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>2008</td>\n",
       "      <td>413562.0</td>\n",
       "      <td>628200</td>\n",
       "      <td>680493</td>\n",
       "      <td>0.658329</td>\n",
       "      <td>0.607739</td>\n",
       "      <td>27.5</td>\n",
       "      <td>883.874</td>\n",
       "      <td>0.67740</td>\n",
       "      <td>37.938640</td>\n",
       "      <td>883.874</td>\n",
       "      <td>1949</td>\n",
       "      <td>453.501283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>2006</td>\n",
       "      <td>1162391.0</td>\n",
       "      <td>2437559</td>\n",
       "      <td>2683538</td>\n",
       "      <td>0.476867</td>\n",
       "      <td>0.433156</td>\n",
       "      <td>33.7</td>\n",
       "      <td>3517.460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3517.460</td>\n",
       "      <td>4842</td>\n",
       "      <td>726.447749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2012</td>\n",
       "      <td>1078548.0</td>\n",
       "      <td>2109847</td>\n",
       "      <td>2242740</td>\n",
       "      <td>0.511197</td>\n",
       "      <td>0.480906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2952.880</td>\n",
       "      <td>0.74210</td>\n",
       "      <td>38.335746</td>\n",
       "      <td>2952.880</td>\n",
       "      <td>52035</td>\n",
       "      <td>56.747958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>2014</td>\n",
       "      <td>1955042.0</td>\n",
       "      <td>6014127</td>\n",
       "      <td>6940888</td>\n",
       "      <td>0.325075</td>\n",
       "      <td>0.281670</td>\n",
       "      <td>37.4</td>\n",
       "      <td>8867.280</td>\n",
       "      <td>0.56750</td>\n",
       "      <td>39.047098</td>\n",
       "      <td>8867.280</td>\n",
       "      <td>7354</td>\n",
       "      <td>1205.776448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>2022</td>\n",
       "      <td>2540666.0</td>\n",
       "      <td>4357176</td>\n",
       "      <td>4642930</td>\n",
       "      <td>0.583099</td>\n",
       "      <td>0.547212</td>\n",
       "      <td>45.9</td>\n",
       "      <td>5841.040</td>\n",
       "      <td>0.64987</td>\n",
       "      <td>39.363628</td>\n",
       "      <td>5841.040</td>\n",
       "      <td>103642</td>\n",
       "      <td>56.357847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>Nebraska</td>\n",
       "      <td>2004</td>\n",
       "      <td>792906.0</td>\n",
       "      <td>1236522</td>\n",
       "      <td>1305430</td>\n",
       "      <td>0.641239</td>\n",
       "      <td>0.607391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1749.370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1749.370</td>\n",
       "      <td>76824</td>\n",
       "      <td>22.771139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           State  Year  TotalBallots  VotingEligiblePopulation  \\\n",
       "0        Alabama  2008     2105622.0                   3454510   \n",
       "1         Alaska  2008      327341.0                    479429   \n",
       "2       Arkansas  2008     1095958.0                   2071563   \n",
       "3       Colorado  2008     2422236.0                   3382959   \n",
       "4       Delaware  2008      413562.0                    628200   \n",
       "..           ...   ...           ...                       ...   \n",
       "607  Connecticut  2006     1162391.0                   2437559   \n",
       "608     Arkansas  2012     1078548.0                   2109847   \n",
       "609   New Jersey  2014     1955042.0                   6014127   \n",
       "610     Colorado  2022     2540666.0                   4357176   \n",
       "611     Nebraska  2004      792906.0                   1236522   \n",
       "\n",
       "     VotingAgePopulation  PercentVotingEligibleVotes  PercentVotingAgeVotes  \\\n",
       "0                3595708                    0.609528               0.585593   \n",
       "1                 507645                    0.682773               0.644823   \n",
       "2                2174846                    0.529049               0.503924   \n",
       "3                3708955                    0.716011               0.653078   \n",
       "4                 680493                    0.658329               0.607739   \n",
       "..                   ...                         ...                    ...   \n",
       "607              2683538                    0.476867               0.433156   \n",
       "608              2242740                    0.511197               0.480906   \n",
       "609              6940888                    0.325075               0.281670   \n",
       "610              4642930                    0.583099               0.547212   \n",
       "611              1305430                    0.641239               0.607391   \n",
       "\n",
       "     PercentBachelors    Income  PercentWhite  AverageAge  Population  \\\n",
       "0                22.0  4718.210       0.68670   37.655371    4718.210   \n",
       "1                27.3   687.455       0.66160   34.414999     687.455   \n",
       "2                18.8  2874.550       0.75710   37.745749    2874.550   \n",
       "3                35.6  4889.730       0.71060   36.314987    4889.730   \n",
       "4                27.5   883.874       0.67740   37.938640     883.874   \n",
       "..                ...       ...           ...         ...         ...   \n",
       "607              33.7  3517.460           NaN         NaN    3517.460   \n",
       "608               NaN  2952.880       0.74210   38.335746    2952.880   \n",
       "609              37.4  8867.280       0.56750   39.047098    8867.280   \n",
       "610              45.9  5841.040       0.64987   39.363628    5841.040   \n",
       "611               NaN  1749.370           NaN         NaN    1749.370   \n",
       "\n",
       "     LandAreaSQM  PopulationDensity  \n",
       "0          50645          93.162405  \n",
       "1         570641           1.204707  \n",
       "2          52035          55.242625  \n",
       "3         103642          47.179039  \n",
       "4           1949         453.501283  \n",
       "..           ...                ...  \n",
       "607         4842         726.447749  \n",
       "608        52035          56.747958  \n",
       "609         7354        1205.776448  \n",
       "610       103642          56.357847  \n",
       "611        76824          22.771139  \n",
       "\n",
       "[612 rows x 14 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging everything together\n",
    "#left join election_year_data with education\n",
    "#note the syntax since we don't want duplicate columns for State,Name \n",
    "# #since they would multiple w/ multiple joins\n",
    "election_demographics = duckdb.sql(\"\"\"SELECT election_year_data.*, PercentBachelors\n",
    "                                   FROM election_year_data\n",
    "                                   LEFT JOIN education ON election_year_data.State = education.Name \n",
    "                                   AND election_year_data.Year = education.Year\"\"\").df()\n",
    "#left join election_demographics with income\n",
    "election_demographics = duckdb.sql(\"\"\"SELECT election_demographics.*, Income\n",
    "                                   FROM election_demographics\n",
    "                                   LEFT JOIN income ON election_demographics.State = income.Name \n",
    "                                   AND election_demographics.Year = income.Year\"\"\").df()\n",
    "#left join election_demographics with diversity\n",
    "election_demographics = duckdb.sql(\"\"\"SELECT election_demographics.*, PercentWhite\n",
    "                                   FROM election_demographics\n",
    "                                   LEFT JOIN diversity \n",
    "                                   ON election_demographics.State = diversity.Location \n",
    "                                   AND election_demographics.Year = diversity.Year\"\"\").df()\n",
    "#left join election_demographics with age\n",
    "election_demographics = duckdb.sql(\"\"\"SELECT election_demographics.*, AverageAge\n",
    "                                   FROM election_demographics\n",
    "                                   LEFT JOIN age ON election_demographics.State = age.Location \n",
    "                                   AND election_demographics.Year = age.Year\"\"\").df()\n",
    "#left join election_demographics with population_density\n",
    "election_demographics = duckdb.sql(\"\"\"SELECT election_demographics.*, \n",
    "                                   Population, LandAreaSQM, PopulationDensity\n",
    "                                   FROM election_demographics\n",
    "                                   LEFT JOIN population_density \n",
    "                                   ON election_demographics.State = population_density.Name \n",
    "                                   AND election_demographics.Year = population_density.Year\"\"\").df()\n",
    "#verify that the number of NAs is appropriate \n",
    "# (since some entire years are missing, thus might appear too high, but there are still plenty of real values)\n",
    "election_demographics.isna().sum()\n",
    "\n",
    "#write it to a csv as a backup\n",
    "#election_demographics.to_csv('data/election_demographics.csv')\n",
    "\n",
    "#print the data to verify accuracy\n",
    "election_demographics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info2950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
